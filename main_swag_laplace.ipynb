{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWAG-Laplace Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datetime import datetime\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from laplace.curvature.asdl import AsdlGGN\n",
    "from laplace.marglik_training import marglik_training\n",
    "from laplace.swag_laplace import SWAGLaplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = './data'\n",
    "BATCH_SIZE = 128\n",
    "LIKELIHOOD = 'classification'\n",
    "EPOCHS = 1\n",
    "MARGLIK_FREQUENCY = 1\n",
    "N_MODELS = 20\n",
    "\n",
    "# Create a directory to save models if it doesn't exist\n",
    "save_dir = './models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Add this to the beginning of your notebook after imports\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "full_dataset = datasets.CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "    inputs, targets = inputs.to(device), targets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Replace the final layer with one that outputs 10 classes for CIFAR-10\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 10)\n",
    "\n",
    "# Either resize inputs or adapt the first layer\n",
    "# Option 1: Resize inputs as shown in Option 2 above\n",
    "# Option 2: Replace the first layer:\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# lap, trained_model, train_loss, val_loss = marglik_training(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     likelihood=LIKELIHOOD,\n",
    "#     n_epochs=EPOCHS,\n",
    "#     marglik_frequency=MARGLIK_FREQUENCY,\n",
    "#     hessian_structure='diag',\n",
    "#     backend=AsdlGGN,\n",
    "#     progress_bar=True,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# model_path = f\"{save_dir}/marglik_trained_model_{timestamp}.pkl\"\n",
    "\n",
    "# with open(model_path, 'wb') as f:\n",
    "#     pickle.dump({\n",
    "#         'model_state': trained_model.state_dict(),\n",
    "#         'training_params': {\n",
    "#             'epochs': EPOCHS,\n",
    "#             'likelihood': LIKELIHOOD,\n",
    "#             'marglik_frequency': MARGLIK_FREQUENCY\n",
    "#         }\n",
    "#     }, f)\n",
    "\n",
    "# print(f\"Marglik trained model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initialize and train SWAG Laplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1/1 [08:23<00:00, 503.03s/it, loss=0.8521]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [] doesn't match the broadcast shape [64, 3, 3, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m swag_laplace = SWAGLaplace(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     likelihood=LIKELIHOOD,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     device=device,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m criterion = nn.CrossEntropyLoss().to(device)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mswag_laplace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m train_accuracy = swag_laplace.evaluate(train_loader)\n\u001b[32m     21\u001b[39m val_accuracy = swag_laplace.evaluate(val_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/University/laplace-v2.0/laplace/swag_laplace.py:111\u001b[39m, in \u001b[36mSWAGLaplace.fit\u001b[39m\u001b[34m(self, train_loader, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# If optimizer and criterion are provided, use SWAG training\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m criterion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_swag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# Fall back to standard Laplace fit method if no optimizer/criterion provided\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28msuper\u001b[39m().fit(train_loader, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/University/laplace-v2.0/laplace/swag_laplace.py:177\u001b[39m, in \u001b[36mSWAGLaplace.train_swag\u001b[39m\u001b[34m(self, train_loader, optimizer, criterion, epochs, start_epoch, progress_bar, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28mself\u001b[39m.swag_covariance = {\n\u001b[32m    171\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvar\u001b[39m\u001b[33m'\u001b[39m: var,\n\u001b[32m    172\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mU\u001b[39m\u001b[33m'\u001b[39m: U,\n\u001b[32m    173\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m: S\n\u001b[32m    174\u001b[39m }\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Compute Laplace approximation using SWAG statistics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_laplace_approximation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/University/laplace-v2.0/laplace/swag_laplace.py:196\u001b[39m, in \u001b[36mSWAGLaplace._compute_laplace_approximation\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# Add diagonal variance to Hessian\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.params):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mH\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# Add low-rank approximation if available\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m S \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# Compute low-rank update to Hessian\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: output with shape [] doesn't match the broadcast shape [64, 3, 3, 3]"
     ]
    }
   ],
   "source": [
    "swag_laplace = SWAGLaplace(\n",
    "    model=model,\n",
    "    likelihood=LIKELIHOOD,\n",
    "    n_models=N_MODELS,\n",
    "    start_epoch=0,\n",
    "    swa_freq=1,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "swag_laplace.fit(\n",
    "    train_loader=train_loader,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    criterion=criterion,\n",
    "    epochs=EPOCHS,\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "train_accuracy = swag_laplace.evaluate(train_loader)\n",
    "val_accuracy = swag_laplace.evaluate(val_loader)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f\"{save_dir}/swag_laplace_model_{timestamp}.pkl\"\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'swag_laplace_state': {\n",
    "            'model_state': model.state_dict(),\n",
    "            'swag_mean': swag_laplace.swag_mean,\n",
    "            'swag_covariance': swag_laplace.swag_covariance,\n",
    "        },\n",
    "        'training_params': {\n",
    "            'epochs': EPOCHS,\n",
    "            'likelihood': LIKELIHOOD,\n",
    "            'n_models': N_MODELS,\n",
    "            'accuracies': {\n",
    "                'train': train_accuracy,\n",
    "                'validation': val_accuracy\n",
    "            }\n",
    "        }\n",
    "    }, f)\n",
    "\n",
    "print(f\"SWAG-Laplace model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get predictions using SWAG-Laplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models = glob.glob(f\"{save_dir}/swag_laplace_model_*.pkl\")\n",
    "\n",
    "if not saved_models:\n",
    "    print(\"No saved models found!\")\n",
    "else:\n",
    "    model_path = max(saved_models, key=os.path.getctime)  # Get the most recent file\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "    # Load the saved model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "\n",
    "    # Create a new instance of your model\n",
    "    loaded_model = resnet18(weights='IMAGENET1K_V1')\n",
    "    loaded_model.load_state_dict(saved_data['swag_laplace_state']['model_state'])\n",
    "    loaded_model = loaded_model.to(device)\n",
    "\n",
    "    # Create a new SWAGLaplace instance\n",
    "    loaded_swag_laplace = SWAGLaplace(\n",
    "        model=loaded_model,\n",
    "        likelihood=saved_data['training_params']['likelihood'],\n",
    "        n_models=saved_data['training_params']['n_models'],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Restore SWAG statistics\n",
    "    loaded_swag_laplace.swag_mean = saved_data['swag_laplace_state']['swag_mean']\n",
    "    loaded_swag_laplace.swag_covariance = saved_data['swag_laplace_state']['swag_covariance']\n",
    "\n",
    "    # Get predictions using loaded model\n",
    "    test_inputs, test_targets = next(iter(val_loader))\n",
    "    test_inputs = test_inputs.to(device)\n",
    "    predictions, uncertainties = loaded_swag_laplace(\n",
    "        test_inputs,\n",
    "        pred_type='glm',\n",
    "        link_approx='mc',\n",
    "        n_samples=100\n",
    "    )\n",
    "\n",
    "    # Retrieve stored accuracies\n",
    "    train_accuracy = saved_data['training_params']['accuracies']['train']\n",
    "    val_accuracy = saved_data['training_params']['accuracies']['validation']\n",
    "    \n",
    "    print(f'Loaded model train accuracy: {train_accuracy:.2f}%')\n",
    "    print(f'Loaded model validation accuracy: {val_accuracy:.2f}%')\n",
    "    print(f'Predictions shape: {predictions.shape}')\n",
    "    print(f'Uncertainties shape: {uncertainties.shape}')\n",
    "    \n",
    "    # Optional: Verify the loaded model's performance\n",
    "    print(\"Evaluating loaded model on validation data...\")\n",
    "    loaded_val_accuracy = loaded_swag_laplace.evaluate(val_loader)\n",
    "    print(f'Re-evaluated validation accuracy: {loaded_val_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
