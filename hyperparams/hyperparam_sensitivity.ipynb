{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ca67f7",
   "metadata": {},
   "source": [
    "# Hyperparameter sensitivity experiment\n",
    "This notebook conducts a post-hoc Laplace approximation hyperparameter sensitivity analysis on pre-trained LeNet5 and 2-layers MLP models trained on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6387e9",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Imports and model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0fbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5 output shape: torch.Size([1, 10])\n",
      "MLP output shape: torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alberto\\AppData\\Local\\Temp\\ipykernel_22992\\1513043869.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pth, map_location=device))\n",
      "C:\\Users\\Alberto\\AppData\\Local\\Temp\\ipykernel_22992\\1513043869.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pth, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Add root directory to sys.path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from laplace.laplace import Laplace\n",
    "from laplace.baselaplace import BaseLaplace, ParametricLaplace, FullLaplace, KronLaplace, LowRankLaplace, DiagLaplace, FunctionalLaplace\n",
    "from laplace.lllaplace import LLLaplace, FunctionalLLLaplace, DiagLLLaplace, FullLLLaplace, KronLLLaplace\n",
    "from models.wideresnet.wideresnet import WideResNet\n",
    "from models.lenet.lenet5 import LeNet5\n",
    "from models.mlp.mlp import MLP\n",
    "from models.widelenet.widelenet import WideLeNet\n",
    "from models.resnet.resnet18 import ResNet18\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seeds = [12, 37, 42]\n",
    "\n",
    "# Instantiate and load models\n",
    "def load_widelenet_models(seeds):\n",
    "    models = []\n",
    "    for seed in seeds:\n",
    "        model = LeNet5()\n",
    "        pth = f\"{project_root}/hyperparams/models/lenet/pretrained/lenet_mnist_seed{seed}.pth\"\n",
    "        model.load_state_dict(torch.load(pth, map_location=device))\n",
    "        model.to(device).eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "def load_mlp_models(seeds):\n",
    "    models = []\n",
    "    for seed in seeds:\n",
    "        model = MLP()\n",
    "        pth = f\"{project_root}/hyperparams/models/mlp/pretrained/mlp_mnist_seed{seed}.pth\"\n",
    "        model.load_state_dict(torch.load(pth, map_location=device))\n",
    "        model.to(device).eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "lenet_models = load_lenet_models(seeds)\n",
    "mlp_models = load_mlp_models(seeds)\n",
    "\n",
    "#check shape\n",
    "x_dummy = torch.randn(1, 1, 28, 28).to(device)  # for MNIST\n",
    "print(\"LeNet5 output shape:\", lenet_models[0](x_dummy).shape)\n",
    "print(\"MLP output shape:\", mlp_models[0](x_dummy.view(1, -1)).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e4339",
   "metadata": {},
   "source": [
    "## 2. Data preparation\n",
    "Load MNIST test set, both ID (MNIST) and OOD (Fashion-MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b91b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# ID - MNIST\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=128, shuffle=False)\n",
    "\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=128, shuffle=False)\n",
    "\n",
    "# OOD - Fashion MNIST\n",
    "fashionmnist_test  = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "fashionmnist_loader = DataLoader(fashionmnist_test, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5f401",
   "metadata": {},
   "source": [
    "## 3. Default settings and hyperparameter options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2771483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp_options = {\n",
    "#     'prior_precision':    [1e-6, 1e-4, 1e-2, 1.0, 100.0], #torch.logspace(-6, 2, 20).tolist()\n",
    "#     'temperature':        [0.1, 0.5, 1.0, 2.0], #torch.logspace(-1, 1, 10).tolist()\n",
    "#     'hessian_structure':  ['diag', 'kron', 'full', 'lowrank', 'gp'],\n",
    "#     'link_approx':        ['probit', 'mc', 'bridge'],  # we'll switch pred_type when sweeping this\n",
    "#     'n_samples':          [32, 128, 512],\n",
    "#     'joint':              [False, True],\n",
    "#     'diagonal_output':    [False, True],\n",
    "#     'pred_type':          ['nn', 'glm', 'gp'],\n",
    "#     'subset_of_weights':  ['last_layer'], # not using 'subnetwork' and 'all'\n",
    "# }\n",
    "\n",
    "# from itertools import product\n",
    "\n",
    "# # Common across all\n",
    "# prior_precisions = [1e-6, 1e-4, 1e-2, 1.0, 100.0]\n",
    "# temperatures = [0.1, 0.5, 1.0, 2.0]\n",
    "# hessian_structures = ['diag', 'kron', 'full']  # 'lowrank' not supported for last_layer\n",
    "# subset_of_weights = ['last_layer']\n",
    "\n",
    "# # pred_type = 'nn' → uses sampling\n",
    "# nn_grid = list(product(\n",
    "#     prior_precisions,\n",
    "#     temperatures,\n",
    "#     hessian_structures,\n",
    "#     ['nn'],        # pred_type\n",
    "#     [128],         # n_samples\n",
    "#     [None],        # joint\n",
    "#     [None]         # diagonal_output\n",
    "# ))\n",
    "\n",
    "# # pred_type = 'glm' → uses analytic mean/variance\n",
    "# glm_grid = list(product(\n",
    "#     prior_precisions,\n",
    "#     temperatures,\n",
    "#     hessian_structures,\n",
    "#     ['glm'],       # pred_type\n",
    "#     [None],        # n_samples\n",
    "#     [False, True], # joint\n",
    "#     [False, True]  # diagonal_output\n",
    "# ))\n",
    "\n",
    "# # pred_type = 'gp' → FunctionalLLLaplace (requires separate setup)\n",
    "# # Not included in default grid since it requires n_subset and a different class\n",
    "\n",
    "# # Combine\n",
    "# full_grid = nn_grid + glm_grid\n",
    "\n",
    "# # Format as list of dicts\n",
    "# hp_grid = [\n",
    "#     {\n",
    "#         'prior_precision': pp,\n",
    "#         'temperature': temp,\n",
    "#         'hessian_structure': hess,\n",
    "#         'pred_type': pred,\n",
    "#         'n_samples': n_samp,\n",
    "#         'joint': joint,\n",
    "#         'diagonal_output': diag_out,\n",
    "#         'subset_of_weights': 'last_layer'\n",
    "#     }\n",
    "#     for pp, temp, hess, pred, n_samp, joint, diag_out in full_grid\n",
    "# ]\n",
    "\n",
    "# print(f\"Total valid configurations: {len(hp_grid)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65be4614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1) prior_precision\\n   - The precision (inverse variance) of the Gaussian prior placed \\n     on the last-layer weights (θ). A higher prior_precision means a stronger \\n     regularization (i.e., the prior is narrower), while a lower value puts \\n     less weight on the prior and more on the data likelihood.\\n   - Example values: 1e-6 (very weak prior), 1e-4, 1e-2, 1.0, 100.0 (very strong prior).\\n\\n2) temperature\\n   - A scaling factor τ applied to the likelihood's Hessian.\\n     Equivalently, it rescales the curvature that the Laplace posterior uses. \\n     - τ < 1.0 “sharpens” the likelihood, making the posterior narrower (underestimate uncertainty).\\n     - τ > 1.0 “flattens” the likelihood, making the posterior wider (overestimate uncertainty).\\n   - Example values: 0.1 (sharper), 0.5, 1.0 (no rescaling), 2.0 (flatter).\\n\\n3) hessian_structure\\n   - Chooses how we approximate or represent the Hessian (second derivative \\n     of the negative log-likelihood) restricted to the last-layer parameters. Only \\n     these options are valid when subset_of_weights='last_layer':\\n     - 'diag' → Diagonal approximation of the Hessian. Keeps only per-weight variances.\\n     - 'kron' → Kronecker-factorized approximation (blockwise over input/output dimensions). \\n                  More accurate than diag but still efficient.\\n     - 'full' → Exact (dense) Hessian on the last-layer. Most accurate but computationally costly.\\n     - 'gp'   → Functional-GP approximation of the last-layer. Internally builds a Gaussian \\n                  process on top of fixed features, using a subset of training points (n_subset). \\n     (Note: 'lowrank' is not valid for last_layer)\\n\\n4) pred_type\\n   - Determines how we build the predictive distribution p(y*|x*) from the Laplace posterior:\\n     - 'nn'  → “Neural‐network sampling” branch. We draw n_samples weight vectors θ ∼ N(θ_MAP, Σ) \\n                and pass each through the network to estimate predictive probabilities. This is \\n                fully Monte Carlo (MC)–based.\\n     - 'glm' → “Generalized linear model” branch. We approximate the last-layer outputs f* as a \\n                Gaussian (mean + variance) and then apply a closed-form (e.g. probit or bridge) \\n                approximation to get p(y*|f*). Can also draw MC samples of f* (if n_samples>1) \\n                but typically uses analytic formulas to compute predictive mean/variance.\\n     - 'gp'  → “Functional-GP” branch (requires hessian_structure='gp'). We treat the last-layer \\n                as an exact Gaussian process on top of fixed features, using n_subset training points \\n                to build a finite GP approximation. Prediction can be analytic or MC-based.\\n\\n5) n_samples\\n   - Number of Monte Carlo samples to draw when pred_type='nn' or when you explicitly \\n     want to draw MC samples in the 'glm' or 'gp' branches. \\n     - If pred_type='nn', n_samples≥1 denotes how many weight draws θ to sample from the Laplace \\n       posterior. Each θ is a full last-layer parameter sample, so n_samples controls sampling variance.\\n     - If pred_type='glm', setting n_samples=1 means “use analytic mean/variance → no sampling.” \\n       If n_samples>1, you first compute f_mu and f_var, then draw f ∼ N(f_mu, f_var) n_samples times \\n       and average predictive probabilities.\\n     - If pred_type='gp', setting n_samples=1 means “use analytic GP predictive.” If n_samples>1, \\n       you can draw posterior samples of f* from the GP to approximate predictive uncertainty.\\n   - Example values for 'nn': [32, 128, 512]. For 'glm' or 'gp', keep n_samples=1 or set >1 when desired.\\n\\n6) link_approx\\n   - Only relevant when pred_type is 'glm' or 'gp'. Chooses how to approximate the \\n     multi-class softmax integral under a Gaussian posterior on f*.\\n     - 'probit' → Extended probit approximation that gives a closed-form estimate of p(y*|f_mu, f_var). Fast and common.\\n     - 'bridge' → “Laplace-bridge” (Dirichlet‐Laplace) approximation to the same integral. \\n       Typically more accurate in high uncertainty regimes but slightly more expensive.\\n     - 'mc'     → Only valid if pred_type='nn'. Means “sample logits → apply softmax → average.” \\n       Not used in the 'glm' or 'gp' branches.\\n\\n7) joint\\n   - Only applies when pred_type='glm' or pred_type='gp'. Controls whether we return \\n     the **full** C×C covariance matrix of the multi-class predictive distribution (joint=True) \\n     or only the diagonal variances (joint=False).  \\n     - If joint=True, the predictive code will compute and optionally sample from the full \\n       covariance across all classes at once (cost ∼ C^2 per input).  \\n     - If joint=False, only the diagonal of that covariance is used → cost ∼ C per input.  \\n     - If pred_type='nn', joint is ignored (set to None to indicate “not applicable”).\\n\\n8) diagonal_output\\n   - Also only applies when pred_type='glm' or 'gp'. Controls whether, after sampling \\n     from a full covariance, you drop off-diagonal terms and only keep per-class variances.  \\n     - If diagonal_output=True, you force the predictive code to treat any per-class covariance as \\n       diagonal (i.e. ignore cross-class covariances).  \\n     - If diagonal_output=False, you keep the full covariance matrix.  \\n     - If pred_type='nn', diagonal_output is ignored (set to None).\\n\\n9) n_subset\\n   - Only used when hessian_structure='gp' (Functional-GP). Specifies how many \\n     training points to randomly subsample (or approximate in blocks) to form the GP’s Gram matrix.  \\n     - Smaller n_subset → cheaper fitting/inference (GP uses a low-rank approximation with n_subset points).  \\n     - Larger n_subset → more accurate GP posterior but higher cost.  \\n     - If hessian_structure!='gp', set n_subset=None (ignored).\\n   - Example values: [256, 512, 1024].\\n\\n10) subset_of_weights\\n    - Determines which parameters of the neural network to place a Laplace posterior on.  \\n      For this entire script, I fix subset_of_weights='last_layer', meaning I only treat the final \\n      linear layer as random under a Gaussian posterior. All earlier layers remain \\n      point estimates (no uncertainty).\\n    - Valid values (in general library): 'all' (full‐network), 'subnetwork' (user‐specified layers), \\n      'last_layer' (only the last linear).  \\n    - By setting 'last_layer', I ensure the Laplace machinery only builds Hessians or GP covariances \\n      over the final layer’s weight and bias.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1) prior_precision\n",
    "   - The precision (inverse variance) of the Gaussian prior placed \n",
    "     on the last-layer weights (θ). A higher prior_precision means a stronger \n",
    "     regularization (i.e., the prior is narrower), while a lower value puts \n",
    "     less weight on the prior and more on the data likelihood.\n",
    "   - Example values: 1e-6 (very weak prior), 1e-4, 1e-2, 1.0, 100.0 (very strong prior).\n",
    "\n",
    "2) temperature\n",
    "   - A scaling factor τ applied to the likelihood's Hessian.\n",
    "     Equivalently, it rescales the curvature that the Laplace posterior uses. \n",
    "     - τ < 1.0 “sharpens” the likelihood, making the posterior narrower (underestimate uncertainty).\n",
    "     - τ > 1.0 “flattens” the likelihood, making the posterior wider (overestimate uncertainty).\n",
    "   - Example values: 0.1 (sharper), 0.5, 1.0 (no rescaling), 2.0 (flatter).\n",
    "\n",
    "3) hessian_structure\n",
    "   - Chooses how we approximate or represent the Hessian (second derivative \n",
    "     of the negative log-likelihood) restricted to the last-layer parameters. Only \n",
    "     these options are valid when subset_of_weights='last_layer':\n",
    "     - 'diag' → Diagonal approximation of the Hessian. Keeps only per-weight variances.\n",
    "     - 'kron' → Kronecker-factorized approximation (blockwise over input/output dimensions). \n",
    "                  More accurate than diag but still efficient.\n",
    "     - 'full' → Exact (dense) Hessian on the last-layer. Most accurate but computationally costly.\n",
    "     - 'gp'   → Functional-GP approximation of the last-layer. Internally builds a Gaussian \n",
    "                  process on top of fixed features, using a subset of training points (n_subset). \n",
    "     (Note: 'lowrank' is not valid for last_layer)\n",
    "\n",
    "4) pred_type\n",
    "   - Determines how we build the predictive distribution p(y*|x*) from the Laplace posterior:\n",
    "     - 'nn'  → “Neural‐network sampling” branch. We draw n_samples weight vectors θ ∼ N(θ_MAP, Σ) \n",
    "                and pass each through the network to estimate predictive probabilities. This is \n",
    "                fully Monte Carlo (MC)–based.\n",
    "     - 'glm' → “Generalized linear model” branch. We approximate the last-layer outputs f* as a \n",
    "                Gaussian (mean + variance) and then apply a closed-form (e.g. probit or bridge) \n",
    "                approximation to get p(y*|f*). Can also draw MC samples of f* (if n_samples>1) \n",
    "                but typically uses analytic formulas to compute predictive mean/variance.\n",
    "     - 'gp'  → “Functional-GP” branch (requires hessian_structure='gp'). We treat the last-layer \n",
    "                as an exact Gaussian process on top of fixed features, using n_subset training points \n",
    "                to build a finite GP approximation. Prediction can be analytic or MC-based.\n",
    "\n",
    "5) n_samples\n",
    "   - Number of Monte Carlo samples to draw when pred_type='nn' or when you explicitly \n",
    "     want to draw MC samples in the 'glm' or 'gp' branches. \n",
    "     - If pred_type='nn', n_samples≥1 denotes how many weight draws θ to sample from the Laplace \n",
    "       posterior. Each θ is a full last-layer parameter sample, so n_samples controls sampling variance.\n",
    "     - If pred_type='glm', setting n_samples=1 means “use analytic mean/variance → no sampling.” \n",
    "       If n_samples>1, you first compute f_mu and f_var, then draw f ∼ N(f_mu, f_var) n_samples times \n",
    "       and average predictive probabilities.\n",
    "     - If pred_type='gp', setting n_samples=1 means “use analytic GP predictive.” If n_samples>1, \n",
    "       you can draw posterior samples of f* from the GP to approximate predictive uncertainty.\n",
    "   - Example values for 'nn': [32, 128, 512]. For 'glm' or 'gp', keep n_samples=1 or set >1 when desired.\n",
    "\n",
    "6) link_approx\n",
    "   - Only relevant when pred_type is 'glm' or 'gp'. Chooses how to approximate the \n",
    "     multi-class softmax integral under a Gaussian posterior on f*.\n",
    "     - 'probit' → Extended probit approximation that gives a closed-form estimate of p(y*|f_mu, f_var). Fast and common.\n",
    "     - 'bridge' → “Laplace-bridge” (Dirichlet‐Laplace) approximation to the same integral. \n",
    "       Typically more accurate in high uncertainty regimes but slightly more expensive.\n",
    "     - 'mc'     → Only valid if pred_type='nn'. Means “sample logits → apply softmax → average.” \n",
    "       Not used in the 'glm' or 'gp' branches.\n",
    "\n",
    "7) joint\n",
    "   - Only applies when pred_type='glm' or pred_type='gp'. Controls whether we return \n",
    "     the **full** C×C covariance matrix of the multi-class predictive distribution (joint=True) \n",
    "     or only the diagonal variances (joint=False).  \n",
    "     - If joint=True, the predictive code will compute and optionally sample from the full \n",
    "       covariance across all classes at once (cost ∼ C^2 per input).  \n",
    "     - If joint=False, only the diagonal of that covariance is used → cost ∼ C per input.  \n",
    "     - If pred_type='nn', joint is ignored (set to None to indicate “not applicable”).\n",
    "\n",
    "8) diagonal_output\n",
    "   - Also only applies when pred_type='glm' or 'gp'. Controls whether, after sampling \n",
    "     from a full covariance, you drop off-diagonal terms and only keep per-class variances.  \n",
    "     - If diagonal_output=True, you force the predictive code to treat any per-class covariance as \n",
    "       diagonal (i.e. ignore cross-class covariances).  \n",
    "     - If diagonal_output=False, you keep the full covariance matrix.  \n",
    "     - If pred_type='nn', diagonal_output is ignored (set to None).\n",
    "\n",
    "9) n_subset\n",
    "   - Only used when hessian_structure='gp' (Functional-GP). Specifies how many \n",
    "     training points to randomly subsample (or approximate in blocks) to form the GP’s Gram matrix.  \n",
    "     - Smaller n_subset → cheaper fitting/inference (GP uses a low-rank approximation with n_subset points).  \n",
    "     - Larger n_subset → more accurate GP posterior but higher cost.  \n",
    "     - If hessian_structure!='gp', set n_subset=None (ignored).\n",
    "   - Example values: [256, 512, 1024].\n",
    "\n",
    "10) subset_of_weights\n",
    "    - Determines which parameters of the neural network to place a Laplace posterior on.  \n",
    "      For this entire script, I fix subset_of_weights='last_layer', meaning I only treat the final \n",
    "      linear layer as random under a Gaussian posterior. All earlier layers remain \n",
    "      point estimates (no uncertainty).\n",
    "    - Valid values (in general library): 'all' (full‐network), 'subnetwork' (user‐specified layers), \n",
    "      'last_layer' (only the last linear).  \n",
    "    - By setting 'last_layer', I ensure the Laplace machinery only builds Hessians or GP covariances \n",
    "      over the final layer’s weight and bias.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c86fab4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid GLM configurations: 64\n"
     ]
    }
   ],
   "source": [
    "# #456 configurations\n",
    "\n",
    "# from itertools import product\n",
    "\n",
    "# # 1. Common hyperparameter values \n",
    "# prior_precisions   = [1e-6, 1e-2, 1.0, 100.0]\n",
    "# temperatures       = [0.5, 1.0]\n",
    "# # For last‐layer non‐GP: use diag/kron/full\n",
    "# hess_structs_main  = ['diag', 'kron', 'full']\n",
    "# # For functional‐GP: only 'gp'\n",
    "# hess_structs_gp    = ['gp']\n",
    "\n",
    "# # 2. Additional options for each pred_type\n",
    "# # 'nn' uses MC sampling\n",
    "# n_samples_nn       = [32, 128, 512]\n",
    "\n",
    "# # 'glm' uses analytic mean/variance plus link‐approx\n",
    "# link_options_glm   = ['probit', 'bridge']\n",
    "# joint_options      = [False, True]\n",
    "# diag_out_options   = [False, True]\n",
    "# # I'll set n_samples=1 for analytic mode\n",
    "\n",
    "# # 'gp' (FunctionalLLLaplace) requires an 'n_subset' choice\n",
    "# n_subset_options   = [256, 512, 1024]\n",
    "# # GP also supports link_approx ∈ {'probit','bridge'} and joint/diagonal_output\n",
    "# # and can do MC‐sampling if n_samples>1 (we’ll leave n_samples=1 here for the analytic predictive)\n",
    "\n",
    "# # 3) Build the hyperparameter grid\n",
    "# hp_grid = []\n",
    "\n",
    "# # pred_type = 'nn'\n",
    "# for pp, temp, hess, n_samp in product(\n",
    "#     prior_precisions,\n",
    "#     temperatures,\n",
    "#     hess_structs_main,\n",
    "#     n_samples_nn\n",
    "# ):\n",
    "#     hp_grid.append({\n",
    "#         'prior_precision':   pp,\n",
    "#         'temperature':       temp,\n",
    "#         'hessian_structure': hess,         # one of 'diag','kron','full'\n",
    "#         'pred_type':         'nn',\n",
    "#         'n_samples':         n_samp,       # Monte Carlo draws\n",
    "#         'link_approx':       'mc',         # forced for 'nn'\n",
    "#         'joint':             None,         # ignored for 'nn'\n",
    "#         'diagonal_output':   None,         # ignored for 'nn'\n",
    "#         'subset_of_weights': 'last_layer'\n",
    "#     })\n",
    "\n",
    "# # pred_type = 'glm'\n",
    "# for pp, temp, hess, link, joint_flag, diag_out in product(\n",
    "#     prior_precisions,\n",
    "#     temperatures,\n",
    "#     hess_structs_main,\n",
    "#     link_options_glm,\n",
    "#     joint_options,\n",
    "#     diag_out_options\n",
    "# ):\n",
    "#     hp_grid.append({\n",
    "#         'prior_precision':   pp,\n",
    "#         'temperature':       temp,\n",
    "#         'hessian_structure': hess,         # one of 'diag','kron','full'\n",
    "#         'pred_type':         'glm',\n",
    "#         'n_samples':         1,            # analytic GLM; set >1 if you want MC‐samples\n",
    "#         'link_approx':       link,         # 'probit' or 'bridge'\n",
    "#         'joint':             joint_flag,   # whether to compute full C×C covariance\n",
    "#         'diagonal_output':   diag_out,     # whether to return only diag of functional‐cov\n",
    "#         'subset_of_weights': 'last_layer'\n",
    "#     })\n",
    "\n",
    "# # pred_type = 'gp'\n",
    "# for pp, temp, n_sub, link, joint_flag, diag_out in product(\n",
    "#     prior_precisions,\n",
    "#     temperatures,\n",
    "#     n_subset_options,\n",
    "#     link_options_glm,   # GP predictive can also choose 'probit' or 'bridge'\n",
    "#     joint_options,\n",
    "#     diag_out_options\n",
    "# ):\n",
    "#     hp_grid.append({\n",
    "#         'prior_precision':   pp,\n",
    "#         'temperature':       temp,\n",
    "#         'hessian_structure': 'gp',         # triggers FunctionalLLLaplace\n",
    "#         'pred_type':         'gp',\n",
    "#         'n_subset':          n_sub,        # # of points to build the GP covariance\n",
    "#         'n_samples':         1,            # analytic GP; set >1 if you want MC‐samples\n",
    "#         'link_approx':       link,         # 'probit' or 'bridge'\n",
    "#         'joint':             joint_flag,   # full functional‐covariance if True\n",
    "#         'diagonal_output':   diag_out,     # use only diagonal of functional‐cov if True\n",
    "#         'subset_of_weights': 'last_layer'\n",
    "#     })\n",
    "\n",
    "# print(f\"Total valid configurations: {len(hp_grid)}\")\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# Common hyperparameter values\n",
    "prior_precisions  = [1e-6, 1e-2, 1.0, 100.0]\n",
    "temperatures      = [0.5, 1.0]\n",
    "# For last‐layer (‘subset_of_weights=last_layer’), we only keep ‘diag’ and ‘kron’\n",
    "hess_structs_main = ['diag', 'kron']\n",
    "\n",
    "# 2. GLM link‐approx options\n",
    "link_options_glm = ['probit', 'bridge']\n",
    "diag_out_options = [False, True]\n",
    "# We only keep joint=False for simplicity (no full output covariance)\n",
    "joint_options = [False]\n",
    "\n",
    "# 3. Build the shrunken hyperparameter grid:\n",
    "#    (no “nn+MC sampling” block)\n",
    "hp_grid = []\n",
    "for pp, temp, hess, link, diag_out, joint in product(\n",
    "        prior_precisions,\n",
    "        temperatures,\n",
    "        hess_structs_main,\n",
    "        link_options_glm,\n",
    "        diag_out_options,\n",
    "        joint_options\n",
    "    ):\n",
    "    hp_grid.append({\n",
    "        'prior_precision':   pp,\n",
    "        'temperature':       temp,\n",
    "        'hessian_structure': hess,         # 'diag' or 'kron'\n",
    "        'pred_type':         'glm',        # always analytic GLM for last‐layer\n",
    "        'n_samples':         1,            # analytic mode\n",
    "        'link_approx':       link,         # 'probit' or 'bridge'\n",
    "        'joint':             joint,        # False\n",
    "        'diagonal_output':   diag_out,     # True/False\n",
    "        'subset_of_weights': 'last_layer'\n",
    "    })\n",
    "\n",
    "print(f\"Total valid GLM configurations: {len(hp_grid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c98949",
   "metadata": {},
   "source": [
    "## 4. Utility: ECE computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37117ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ece(probs: torch.Tensor, labels: torch.Tensor, n_bins: int = 15) -> float:\n",
    "    \"\"\"\n",
    "    Compute expected calibration error (ECE) for classification.\n",
    "    \"\"\"\n",
    "    confidences, predictions = torch.max(probs, dim=1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "    bins = torch.linspace(0, 1, n_bins + 1, device=probs.device)\n",
    "    ece = torch.zeros(1, device=probs.device)\n",
    "    for i in range(n_bins):\n",
    "        # find predictions with confidence in (bins[i], bins[i+1]]\n",
    "        mask = (confidences > bins[i]) & (confidences <= bins[i + 1])\n",
    "        if mask.any():\n",
    "            # weight by fraction of data points in this bin\n",
    "            ece += (mask.float().mean() * \n",
    "                    torch.abs(accuracies[mask].float().mean() - confidences[mask].mean()))\n",
    "    return ece.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504702f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/128] lenet | pp=1e-06, τ=0.5, hess=diag, link=probit, diag_out=False  →  ID_acc=0.9787±0.0017, ID_ECE=0.0024±0.0001  |  OOD_acc=0.1077±0.0062, OOD_ECE=0.5556±0.0722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 300\u001b[0m\n\u001b[0;32m    289\u001b[0m la \u001b[38;5;241m=\u001b[39m Laplace(\n\u001b[0;32m    290\u001b[0m     model_copy,\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m     enable_backprop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    297\u001b[0m )\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# Fit on the original MNIST training set\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[43mla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmnist_train_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# -- Evaluate ID (MNIST test) --\u001b[39;00m\n\u001b[0;32m    303\u001b[0m all_probs_id  \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\laplace-v2.0\\laplace\\lllaplace.py:204\u001b[0m, in \u001b[0;36mLLLaplace.fit\u001b[1;34m(self, train_loader, override, progress_bar)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_mean: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prior_mean\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_H()\n\u001b[1;32m--> 204\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m parameters_to_vector(\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[0;32m    207\u001b[0m )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_backprop:\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\laplace-v2.0\\laplace\\baselaplace.py:983\u001b[0m, in \u001b[0;36mParametricLaplace.fit\u001b[1;34m(self, train_loader, override, progress_bar)\u001b[0m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms output has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dims but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    979\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe target has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dims.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 983\u001b[0m loss_batch, H_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_curv_closure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_batch\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m H_batch\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\laplace-v2.0\\laplace\\baselaplace.py:2069\u001b[0m, in \u001b[0;36mDiagLaplace._curv_closure\u001b[1;34m(self, X, y, N)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_curv_closure\u001b[39m(\n\u001b[0;32m   2064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2065\u001b[0m     X: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m MutableMapping[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m Any],\n\u001b[0;32m   2066\u001b[0m     y: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   2067\u001b[0m     N: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   2068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m-> 2069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asdl_fisher_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\laplace-v2.0\\laplace\\curvature\\curvature.py:419\u001b[0m, in \u001b[0;36mGGNInterface.diag\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdiag\u001b[39m(\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    415\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m MutableMapping[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m Any],\n\u001b[0;32m    416\u001b[0m     y: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    418\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 419\u001b[0m     Js, f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_layer_jacobians\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjacobians(x)\n\u001b[0;32m    420\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlossfunc(f, y)\n\u001b[0;32m    422\u001b[0m     H_lik \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_mc_functional_fisher(f)\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_functional_hessian(f)\n\u001b[0;32m    426\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\laplace-v2.0\\laplace\\curvature\\curvature.py:151\u001b[0m, in \u001b[0;36mCurvatureInterface.last_layer_jacobians\u001b[1;34m(self, x, enable_backprop)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlast_layer_jacobians\u001b[39m(\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    133\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m MutableMapping[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m Any],\n\u001b[0;32m    134\u001b[0m     enable_backprop: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    135\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m        output function `(batch, outputs)`\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     f, phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     bsize \u001b[38;5;241m=\u001b[39m phi\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    153\u001b[0m     output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(f\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m/\u001b[39m bsize)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\laplace-v2.0\\laplace\\utils\\feature_extractor.py:109\u001b[0m, in \u001b[0;36mFeatureExtractor.forward_with_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_with_features\u001b[39m(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m MutableMapping[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m Any]\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass which returns the output of the penultimate layer along\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    with the output of the last layer. If the last layer is not known yet,\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    it will be determined when this function is called for the first time.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m        one batch of data to use as input for the forward pass\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_layer_name]\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_reduction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\laplace-v2.0\\laplace\\utils\\feature_extractor.py:94\u001b[0m, in \u001b[0;36mFeatureExtractor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     91\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_last_layer(x)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# if last and penultimate layers are already known\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\Documents\\GitHub\\laplace-v2.0\\hyperparams\\models\\lenet\\lenet5.py:26\u001b[0m, in \u001b[0;36mLeNet5.forward\u001b[1;34m(self, classes)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, classes):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# results_id = []\n",
    "# results_ood = []\n",
    "\n",
    "# arch_model_lists = {\n",
    "#     \"lenet\": lenet_models,\n",
    "#     \"mlp\":   mlp_models,\n",
    "# }\n",
    "\n",
    "# # Freeze gradients\n",
    "# for model_list in arch_model_lists.values():\n",
    "#     for m in model_list:\n",
    "#         m.requires_grad_(False)\n",
    "\n",
    "# # Compute total number of (architecture × hyperparameter) configurations\n",
    "# num_archs = len(arch_model_lists)\n",
    "# num_hps   = len(hp_grid)\n",
    "# total_configs = num_archs * num_hps\n",
    "# config_counter = 0\n",
    "\n",
    "# # Loop over architectures and hyperparameter configurations\n",
    "# for model_name, models_list in arch_model_lists.items():\n",
    "#     for config in hp_grid:\n",
    "#         pp          = config['prior_precision']\n",
    "#         temp        = config['temperature']\n",
    "#         hess        = config['hessian_structure']\n",
    "#         pred_type   = config['pred_type']\n",
    "#         n_samp      = config.get('n_samples', None)\n",
    "#         link        = config.get('link_approx', None)\n",
    "#         joint_flag  = config.get('joint', None)\n",
    "#         diag_out    = config.get('diagonal_output', None)\n",
    "#         n_sub       = config.get('n_subset', None)\n",
    "\n",
    "#         # Accumulators for metrics across seeds\n",
    "#         acc_id_list  = []\n",
    "#         ece_id_list  = []\n",
    "#         acc_ood_list = []\n",
    "#         ece_ood_list = []\n",
    "\n",
    "#         # Loop over seeds\n",
    "#         for base_model in models_list:\n",
    "#             # Copy pretrained model\n",
    "#             model_copy = type(base_model)().to(device)\n",
    "#             model_copy.load_state_dict(base_model.state_dict())\n",
    "#             model_copy.eval()\n",
    "\n",
    "#             # Instantiate Laplace or FunctionalLLLaplace\n",
    "#             if hess == 'gp':\n",
    "#                 la = FunctionalLLLaplace(\n",
    "#                     model_copy,\n",
    "#                     'classification',\n",
    "#                     n_subset=n_sub,\n",
    "#                     prior_precision=pp,\n",
    "#                     temperature=temp,\n",
    "#                     enable_backprop=False\n",
    "#                 )\n",
    "#             else:\n",
    "#                 la = Laplace(\n",
    "#                     model_copy,\n",
    "#                     'classification',\n",
    "#                     subset_of_weights='last_layer',\n",
    "#                     hessian_structure=hess,\n",
    "#                     prior_precision=pp,\n",
    "#                     temperature=temp,\n",
    "#                     enable_backprop=False\n",
    "#                 )\n",
    "\n",
    "#             # Fit on MNIST train\n",
    "#             la.fit(mnist_train_loader)\n",
    "\n",
    "#             # Evaluate ID\n",
    "#             all_probs_id  = []\n",
    "#             all_labels_id = []\n",
    "#             with torch.no_grad():\n",
    "#                 for X, y in mnist_test_loader:\n",
    "#                     X, y = X.to(device), y.to(device)\n",
    "\n",
    "#                     if pred_type == 'nn':\n",
    "#                         probs = la(\n",
    "#                             X,\n",
    "#                             pred_type='nn',\n",
    "#                             link_approx='mc',\n",
    "#                             n_samples=n_samp\n",
    "#                         )\n",
    "#                     elif pred_type == 'glm':\n",
    "#                         probs = la(\n",
    "#                             X,\n",
    "#                             pred_type='glm',\n",
    "#                             joint=joint_flag,\n",
    "#                             link_approx=link,\n",
    "#                             n_samples=1,\n",
    "#                             diagonal_output=diag_out\n",
    "#                         )\n",
    "#                     else:  # 'gp'\n",
    "#                         probs = la(\n",
    "#                             X,\n",
    "#                             pred_type='gp',\n",
    "#                             joint=joint_flag,\n",
    "#                             link_approx=link,\n",
    "#                             n_samples=1,\n",
    "#                             diagonal_output=diag_out\n",
    "#                         )\n",
    "\n",
    "#                     all_probs_id.append(probs.cpu())\n",
    "#                     all_labels_id.append(y.cpu())\n",
    "\n",
    "#             all_probs_id  = torch.cat(all_probs_id, dim=0)\n",
    "#             all_labels_id = torch.cat(all_labels_id, dim=0)\n",
    "\n",
    "#             _, preds_id = torch.max(all_probs_id, dim=1)\n",
    "#             acc_id = (preds_id == all_labels_id).float().mean().item()\n",
    "#             ece_id = compute_ece(all_probs_id, all_labels_id)\n",
    "\n",
    "#             # Evaluate OOD\n",
    "#             all_probs_ood  = []\n",
    "#             all_labels_ood = []\n",
    "#             with torch.no_grad():\n",
    "#                 for X, y in fashionmnist_loader:\n",
    "#                     X, y = X.to(device), y.to(device)\n",
    "\n",
    "#                     if pred_type == 'nn':\n",
    "#                         probs = la(\n",
    "#                             X,\n",
    "#                             pred_type='nn',\n",
    "#                             link_approx='mc',\n",
    "#                             n_samples=n_samp\n",
    "#                         )\n",
    "#                     elif pred_type == 'glm':\n",
    "#                         probs = la(\n",
    "#                             X,\n",
    "#                             pred_type='glm',\n",
    "#                             joint=joint_flag,\n",
    "#                             link_approx=link,\n",
    "#                             n_samples=1,\n",
    "#                             diagonal_output=diag_out\n",
    "#                         )\n",
    "#                     else:  # 'gp'\n",
    "#                         probs = la(\n",
    "#                             X,\n",
    "#                             pred_type='gp',\n",
    "#                             joint=joint_flag,\n",
    "#                             link_approx=link,\n",
    "#                             n_samples=1,\n",
    "#                             diagonal_output=diag_out\n",
    "#                         )\n",
    "\n",
    "#                     all_probs_ood.append(probs.cpu())\n",
    "#                     all_labels_ood.append(y.cpu())\n",
    "\n",
    "#             all_probs_ood  = torch.cat(all_probs_ood, dim=0)\n",
    "#             all_labels_ood = torch.cat(all_labels_ood, dim=0)\n",
    "\n",
    "#             _, preds_ood = torch.max(all_probs_ood, dim=1)\n",
    "#             acc_ood = (preds_ood == all_labels_ood).float().mean().item()\n",
    "#             ece_ood = compute_ece(all_probs_ood, all_labels_ood)\n",
    "\n",
    "#             # Collect this seed’s metrics\n",
    "#             acc_id_list.append(acc_id)\n",
    "#             ece_id_list.append(ece_id)\n",
    "#             acc_ood_list.append(acc_ood)\n",
    "#             ece_ood_list.append(ece_ood)\n",
    "\n",
    "#             # Free memory\n",
    "#             del la, model_copy, all_probs_id, all_probs_ood\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         # Compute mean and std across seeds\n",
    "#         acc_id_mean  = float(torch.tensor(acc_id_list).mean().item())\n",
    "#         acc_id_std   = float(torch.tensor(acc_id_list).std(unbiased=False).item())\n",
    "#         ece_id_mean  = float(torch.tensor(ece_id_list).mean().item())\n",
    "#         ece_id_std   = float(torch.tensor(ece_id_list).std(unbiased=False).item())\n",
    "\n",
    "#         acc_ood_mean = float(torch.tensor(acc_ood_list).mean().item())\n",
    "#         acc_ood_std  = float(torch.tensor(acc_ood_list).std(unbiased=False).item())\n",
    "#         ece_ood_mean = float(torch.tensor(ece_ood_list).mean().item())\n",
    "#         ece_ood_std  = float(torch.tensor(ece_ood_list).std(unbiased=False).item())\n",
    "\n",
    "#         # Record ID metrics in one list and OOD in another\n",
    "#         row_id = {\n",
    "#             \"model_type\":        model_name,\n",
    "#             \"prior_precision\":   pp,\n",
    "#             \"temperature\":       temp,\n",
    "#             \"hessian_structure\": hess,\n",
    "#             \"pred_type\":         pred_type,\n",
    "#             \"n_samples\":         n_samp if pred_type == 'nn' else None,\n",
    "#             \"link_approx\":       link,\n",
    "#             \"joint\":             joint_flag,\n",
    "#             \"diagonal_output\":   diag_out,\n",
    "#             \"n_subset\":          n_sub if hess == 'gp' else None,\n",
    "#             \"acc_id_mean\":       acc_id_mean,\n",
    "#             \"acc_id_std\":        acc_id_std,\n",
    "#             \"ece_id_mean\":       ece_id_mean,\n",
    "#             \"ece_id_std\":        ece_id_std,\n",
    "#         }\n",
    "#         results_id.append(row_id)\n",
    "\n",
    "#         row_ood = {\n",
    "#             \"model_type\":         model_name,\n",
    "#             \"prior_precision\":    pp,\n",
    "#             \"temperature\":        temp,\n",
    "#             \"hessian_structure\":  hess,\n",
    "#             \"pred_type\":          pred_type,\n",
    "#             \"n_samples\":          n_samp if pred_type == 'nn' else None,\n",
    "#             \"link_approx\":        link,\n",
    "#             \"joint\":              joint_flag,\n",
    "#             \"diagonal_output\":    diag_out,\n",
    "#             \"n_subset\":           n_sub if hess == 'gp' else None,\n",
    "#             \"acc_ood_mean\":       acc_ood_mean,\n",
    "#             \"acc_ood_std\":        acc_ood_std,\n",
    "#             \"ece_ood_mean\":       ece_ood_mean,\n",
    "#             \"ece_ood_std\":        ece_ood_std,\n",
    "#         }\n",
    "#         results_ood.append(row_ood)\n",
    "\n",
    "#         # Increment counter, print progress, and flush every 100 configs\n",
    "#         config_counter += 1\n",
    "#         print(f\"[{config_counter}/{total_configs}] \"\n",
    "#               f\"Finished {model_name} | hp = \"\n",
    "#               f\"(pp={pp}, τ={temp}, hess={hess}, pred={pred_type}, \"\n",
    "#               f\"n_samp={n_samp}, link={link}, joint={joint_flag}, \"\n",
    "#               f\"diag_out={diag_out}, n_sub={n_sub})\")\n",
    "\n",
    "#         # Every 100 configurations, write partial CSVs to disk\n",
    "#         if config_counter % 100 == 0:\n",
    "#             df_id  = pd.DataFrame(results_id)\n",
    "#             df_ood = pd.DataFrame(results_ood)\n",
    "#             df_id.to_csv(os.path.join(project_root, \"last_layer_laplace_metrics_id.csv\"), index=False)\n",
    "#             df_ood.to_csv(os.path.join(project_root, \"last_layer_laplace_metrics_ood.csv\"), index=False)\n",
    "#             print(f\"  → Flushed {config_counter} rows to CSV files.\")\n",
    "\n",
    "# # After all loops, write final CSVs\n",
    "# df_id_final  = pd.DataFrame(results_id)\n",
    "# df_ood_final = pd.DataFrame(results_ood)\n",
    "# df_id_final.to_csv(os.path.join(project_root, \"last_layer_laplace_metrics_id.csv\"), index=False)\n",
    "# df_ood_final.to_csv(os.path.join(project_root, \"last_layer_laplace_metrics_ood.csv\"), index=False)\n",
    "# print(\"Saved final CSVs for ID and OOD metrics.\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "results_id = []\n",
    "results_ood = []\n",
    "\n",
    "arch_model_lists = {\n",
    "    \"lenet\": lenet_models,\n",
    "    \"mlp\":   mlp_models,\n",
    "}\n",
    "\n",
    "# Prepare an output directory\n",
    "output_dir = Path(\"/kaggle/working/laplace_outputs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Freeze gradients\n",
    "for model_list in arch_model_lists.values():\n",
    "    for m in model_list:\n",
    "        m.requires_grad_(False)\n",
    "\n",
    "# Compute total number of (architecture × hyperparameter) configurations\n",
    "num_archs = len(arch_model_lists)\n",
    "num_hps   = len(hp_grid)\n",
    "total_configs = num_archs * num_hps\n",
    "config_counter = 0\n",
    "\n",
    "for model_name, models_list in arch_model_lists.items():\n",
    "    for config in hp_grid:\n",
    "        pp         = config['prior_precision']\n",
    "        temp       = config['temperature']\n",
    "        hess       = config['hessian_structure']\n",
    "        link       = config['link_approx']\n",
    "        joint_flag = config['joint']\n",
    "        diag_out   = config['diagonal_output']\n",
    "\n",
    "        # Collect per‐seed metrics (though we always use analytic GLM, so each seed is just one run)\n",
    "        acc_id_list  = []\n",
    "        ece_id_list  = []\n",
    "        acc_ood_list = []\n",
    "        ece_ood_list = []\n",
    "\n",
    "        # Loop over seeds\n",
    "        for base_model in models_list:\n",
    "            # Copy pretrained model into a fresh instance\n",
    "            model_copy = type(base_model)().to(device)\n",
    "            model_copy.load_state_dict(base_model.state_dict())\n",
    "            model_copy.eval()\n",
    "\n",
    "            # Build a Laplace object (analytic GLM on last layer)\n",
    "            la = Laplace(\n",
    "                model_copy,\n",
    "                'classification',\n",
    "                subset_of_weights='last_layer',\n",
    "                hessian_structure=hess,\n",
    "                prior_precision=pp,\n",
    "                temperature=temp,\n",
    "                enable_backprop=False\n",
    "            )\n",
    "\n",
    "            # Fit on the original MNIST training set\n",
    "            la.fit(mnist_train_loader)\n",
    "\n",
    "            # -- Evaluate ID (MNIST test) --\n",
    "            all_probs_id  = []\n",
    "            all_labels_id = []\n",
    "            skip_this_hp = False\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for X, y in mnist_test_loader:\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    try:\n",
    "                        probs = la(\n",
    "                            X,\n",
    "                            pred_type='glm',\n",
    "                            link_approx=link,\n",
    "                            n_samples=1,\n",
    "                            diagonal_output=diag_out\n",
    "                        )\n",
    "                    except (RuntimeError, torch._C._LinAlgError):\n",
    "                        # If Hessian not PD → skip entire ID/OOD for this seed+HP\n",
    "                        skip_this_hp = True\n",
    "                        break\n",
    "\n",
    "                    all_probs_id.append(probs.cpu())\n",
    "                    all_labels_id.append(y.cpu())\n",
    "\n",
    "            if skip_this_hp or len(all_probs_id)==0:\n",
    "                # non‐PD Hessian → record NaN\n",
    "                acc_id = float('nan')\n",
    "                ece_id = float('nan')\n",
    "            else:\n",
    "                all_probs_id  = torch.cat(all_probs_id, dim=0)\n",
    "                all_labels_id = torch.cat(all_labels_id, dim=0)\n",
    "                _, preds_id   = torch.max(all_probs_id, dim=1)\n",
    "                acc_id        = (preds_id == all_labels_id).float().mean().item()\n",
    "                ece_id        = compute_ece(all_probs_id, all_labels_id)\n",
    "\n",
    "            # -- Evaluate OOD (FashionMNIST) --\n",
    "            all_probs_ood  = []\n",
    "            all_labels_ood = []\n",
    "            if skip_this_hp:\n",
    "                acc_ood = float('nan')\n",
    "                ece_ood = float('nan')\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    for X, y in fashionmnist_loader:\n",
    "                        X, y = X.to(device), y.to(device)\n",
    "                        try:\n",
    "                            probs = la(\n",
    "                                X,\n",
    "                                pred_type='glm',\n",
    "                                link_approx=link,\n",
    "                                n_samples=1,\n",
    "                                diagonal_output=diag_out\n",
    "                            )\n",
    "                        except (RuntimeError, torch._C._LinAlgError):\n",
    "                            skip_this_hp = True\n",
    "                            break\n",
    "\n",
    "                        all_probs_ood.append(probs.cpu())\n",
    "                        all_labels_ood.append(y.cpu())\n",
    "\n",
    "                if skip_this_hp or len(all_probs_ood)==0:\n",
    "                    acc_ood = float('nan')\n",
    "                    ece_ood = float('nan')\n",
    "                else:\n",
    "                    all_probs_ood  = torch.cat(all_probs_ood, dim=0)\n",
    "                    all_labels_ood = torch.cat(all_labels_ood, dim=0)\n",
    "                    _, preds_ood   = torch.max(all_probs_ood, dim=1)\n",
    "                    acc_ood        = (preds_ood == all_labels_ood).float().mean().item()\n",
    "                    ece_ood        = compute_ece(all_probs_ood, all_labels_ood)\n",
    "\n",
    "            # Collect this seed’s results\n",
    "            acc_id_list.append(acc_id)\n",
    "            ece_id_list.append(ece_id)\n",
    "            acc_ood_list.append(acc_ood)\n",
    "            ece_ood_list.append(ece_ood)\n",
    "\n",
    "            # Clean up GPU memory for this seed\n",
    "            del la, model_copy, all_probs_id, all_probs_ood\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Compute nan‐aware mean and std via module‐level torch.nanmean / torch.nanstd\n",
    "        t_acc_id  = torch.tensor(acc_id_list,  dtype=torch.float32, device='cpu')\n",
    "        valid_acc_id = t_acc_id[~torch.isnan(t_acc_id)]\n",
    "        acc_id_mean = valid_acc_id.mean().item()\n",
    "        acc_id_std  = valid_acc_id.std(unbiased=False).item()\n",
    "        \n",
    "        t_ece_id  = torch.tensor(ece_id_list,  dtype=torch.float32, device='cpu')\n",
    "        valid_ece_id = t_ece_id[~torch.isnan(t_ece_id)]\n",
    "        ece_id_mean = valid_ece_id.mean().item()\n",
    "        ece_id_std  = valid_ece_id.std(unbiased=False).item()\n",
    "        \n",
    "        t_acc_ood = torch.tensor(acc_ood_list, dtype=torch.float32, device='cpu')\n",
    "        valid_acc_ood = t_acc_ood[~torch.isnan(t_acc_ood)]\n",
    "        acc_ood_mean = valid_acc_ood.mean().item()\n",
    "        acc_ood_std  = valid_acc_ood.std(unbiased=False).item()\n",
    "        \n",
    "        t_ece_ood = torch.tensor(ece_ood_list, dtype=torch.float32, device='cpu')\n",
    "        valid_ece_ood = t_ece_ood[~torch.isnan(t_ece_ood)]\n",
    "        ece_ood_mean = valid_ece_ood.mean().item()\n",
    "        ece_ood_std  = valid_ece_ood.std(unbiased=False).item()\n",
    "        \n",
    "        # Record one row for ID and one for OOD\n",
    "        results_id.append({\n",
    "            \"model_type\":        model_name,\n",
    "            \"prior_precision\":   pp,\n",
    "            \"temperature\":       temp,\n",
    "            \"hessian_structure\": hess,\n",
    "            \"link_approx\":       link,\n",
    "            \"joint\":             joint_flag,\n",
    "            \"diagonal_output\":   diag_out,\n",
    "            \"acc_id_mean\":       acc_id_mean,\n",
    "            \"acc_id_std\":        acc_id_std,\n",
    "            \"ece_id_mean\":       ece_id_mean,\n",
    "            \"ece_id_std\":        ece_id_std,\n",
    "        })\n",
    "\n",
    "        results_ood.append({\n",
    "            \"model_type\":        model_name,\n",
    "            \"prior_precision\":   pp,\n",
    "            \"temperature\":       temp,\n",
    "            \"hessian_structure\": hess,\n",
    "            \"link_approx\":       link,\n",
    "            \"joint\":             joint_flag,\n",
    "            \"diagonal_output\":   diag_out,\n",
    "            \"acc_ood_mean\":      acc_ood_mean,\n",
    "            \"acc_ood_std\":       acc_ood_std,\n",
    "            \"ece_ood_mean\":      ece_ood_mean,\n",
    "            \"ece_ood_std\":       ece_ood_std,\n",
    "        })\n",
    "\n",
    "        config_counter += 1\n",
    "        print(f\"[{config_counter}/{total_configs}] \"\n",
    "              f\"{model_name} | pp={pp}, τ={temp}, hess={hess}, link={link}, \"\n",
    "              f\"diag_out={diag_out}  →  \"\n",
    "              f\"ID_acc={acc_id_mean:.4f}±{acc_id_std:.4f}, ID_ECE={ece_id_mean:.4f}±{ece_id_std:.4f}  |  \"\n",
    "              f\"OOD_acc={acc_ood_mean:.4f}±{acc_ood_std:.4f}, OOD_ECE={ece_ood_mean:.4f}±{ece_ood_std:.4f}\")\n",
    "\n",
    "       \n",
    "\n",
    "        # Flush every 50 configurations (optional)\n",
    "        if config_counter % 20 == 0:\n",
    "            df_id  = pd.DataFrame(results_id)\n",
    "            df_ood = pd.DataFrame(results_ood)\n",
    "            df_id.to_csv(\n",
    "                os.path.join(output_dir / \"last_layer_laplace_metrics_id.csv\"),\n",
    "                index=False\n",
    "            )\n",
    "            df_ood.to_csv(\n",
    "                os.path.join(output_dir / \"last_layer_laplace_metrics_ood.csv\"),\n",
    "                index=False\n",
    "            )\n",
    "            print(f\"  → Flushed {config_counter} rows to CSV.\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────\n",
    "# 3) Save the final CSVs\n",
    "# ───────────────────────────────────────────────────────────────────\n",
    "df_id_final  = pd.DataFrame(results_id)\n",
    "df_ood_final = pd.DataFrame(results_ood)\n",
    "df_id_final.to_csv(output_dir / \"last_layer_laplace_metrics_id.csv\", index=False)\n",
    "df_ood_final.to_csv(output_dir / \"last_layer_laplace_metrics_ood.csv\", index=False)\n",
    "print(\"Saved final CSVs for ID and OOD metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27959a",
   "metadata": {},
   "source": [
    "## 6. Plotting extended sensitivity results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
