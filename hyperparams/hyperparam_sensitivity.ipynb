{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ca67f7",
   "metadata": {},
   "source": [
    "# Hyperparameter sensitivity experiment\n",
    "This notebook conducts a post-hoc Laplace approximation hyperparameter sensitivity analysis on pre-trained LeNet5 and 2-layers MLP models trained on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6387e9",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Imports and model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f0fbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5 output shape: torch.Size([1, 10])\n",
      "MLP output shape: torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alberto\\AppData\\Local\\Temp\\ipykernel_10060\\4247580706.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pth, map_location=device))\n",
      "C:\\Users\\Alberto\\AppData\\Local\\Temp\\ipykernel_10060\\4247580706.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pth, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Add root directory to sys.path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from laplace.laplace import Laplace\n",
    "from laplace.baselaplace import BaseLaplace, ParametricLaplace, FullLaplace, KronLaplace, LowRankLaplace, DiagLaplace, FunctionalLaplace\n",
    "from laplace.lllaplace import LLLaplace, FunctionalLLLaplace, DiagLLLaplace, FullLLLaplace, KronLLLaplace\n",
    "from models.wideresnet.wideresnet import WideResNet\n",
    "from models.lenet.lenet5 import LeNet5\n",
    "from models.mlp.mlp import MLP\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seeds = [6, 12, 37, 42, 113]\n",
    "\n",
    "# Instantiate and load models\n",
    "def load_lenet_models(seeds):\n",
    "    models = []\n",
    "    for seed in seeds:\n",
    "        model = LeNet5()\n",
    "        pth = f\"{project_root}/hyperparams/models/lenet/pretrained/lenet_mnist_seed{seed}.pth\"\n",
    "        model.load_state_dict(torch.load(pth, map_location=device))\n",
    "        model.to(device).eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "def load_mlp_models(seeds):\n",
    "    models = []\n",
    "    for seed in seeds:\n",
    "        model = MLP()\n",
    "        pth = f\"{project_root}/hyperparams/models/mlp/pretrained/mlp_mnist_seed{seed}.pth\"\n",
    "        model.load_state_dict(torch.load(pth, map_location=device))\n",
    "        model.to(device).eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "lenet_models = load_lenet_models(seeds)\n",
    "mlp_models = load_mlp_models(seeds)\n",
    "\n",
    "#check shape\n",
    "x_dummy = torch.randn(1, 1, 28, 28).to(device)  # for MNIST\n",
    "print(\"LeNet5 output shape:\", lenet_models[0](x_dummy).shape)\n",
    "print(\"MLP output shape:\", mlp_models[0](x_dummy.view(1, -1)).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e4339",
   "metadata": {},
   "source": [
    "## 2. Data preparation\n",
    "Load MNIST test set, both ID (MNIST) and OOD (Fashion-MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b91b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# ID - MNIST\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=128, shuffle=False)\n",
    "\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=128, shuffle=False)\n",
    "\n",
    "# OOD - Fashion MNIST\n",
    "fashionmnist_test  = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "fashionmnist_loader = DataLoader(fashionmnist_test, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5f401",
   "metadata": {},
   "source": [
    "## 3. Default settings and hyperparameter options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2771483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid configurations: 300\n"
     ]
    }
   ],
   "source": [
    "# Default settings for one‐at‐a‐time sweeps\n",
    "'''\n",
    "Key Hyperparameters\n",
    "- prior_precision: Controls the strength of the Gaussian prior over weights.\n",
    "                   Higher values imply stronger regularization.\n",
    "                    Accepts:\n",
    "                        Scalar (most common)\n",
    "                        Per-parameter vector (for advanced use)\n",
    "- temperature: Scales the log-likelihood, which affects posterior uncertainty.\n",
    "               <1 makes the posterior sharper, >1 more diffuse.\n",
    "               likelihood:\n",
    "                  Either classification or regression.\n",
    "                  Affects loss used in curvature estimation.\n",
    "- hessian_structure: 'diag', 'kron', 'full', etc.\n",
    "                      Major impact on runtime and accuracy.\n",
    "                      Must be one of the supported structures in lllaplace.py.\n",
    "\n",
    "- subset_of_weights: I only use 'last_layer'.\n",
    "                     Others ('subnetwork', 'all') are available but much more expensive.\n",
    "\n",
    "- backend_kwargs: For curvature approximation (e.g., using BackPACK, GGN).\n",
    "                  Important for stochastic curvature approximations.\n",
    "'''\n",
    "    \n",
    "# hp_options = {\n",
    "#     'prior_precision':    [1e-6, 1e-4, 1e-2, 1.0, 100.0], #torch.logspace(-6, 2, 20).tolist()\n",
    "#     'temperature':        [0.1, 0.5, 1.0, 2.0], #torch.logspace(-1, 1, 10).tolist()\n",
    "#     'hessian_structure':  ['diag', 'kron', 'full', 'lowrank', 'gp'],\n",
    "#     'link_approx':        ['probit', 'mc', 'bridge'],  # we'll switch pred_type when sweeping this\n",
    "#     'n_samples':          [32, 128, 512],\n",
    "#     'joint':              [False, True],\n",
    "#     'diagonal_output':    [False, True],\n",
    "#     'pred_type':          ['nn', 'glm', 'gp'],\n",
    "#     'subset_of_weights':  ['last_layer'], # not using 'subnetwork' and 'all'\n",
    "# }\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# Common across all\n",
    "prior_precisions = [1e-6, 1e-4, 1e-2, 1.0, 100.0]\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0]\n",
    "hessian_structures = ['diag', 'kron', 'full']  # 'lowrank' not supported for last_layer\n",
    "subset_of_weights = ['last_layer']\n",
    "\n",
    "# pred_type = 'nn' → uses sampling\n",
    "nn_grid = list(product(\n",
    "    prior_precisions,\n",
    "    temperatures,\n",
    "    hessian_structures,\n",
    "    ['nn'],        # pred_type\n",
    "    [128],         # n_samples\n",
    "    [None],        # joint\n",
    "    [None]         # diagonal_output\n",
    "))\n",
    "\n",
    "# pred_type = 'glm' → uses analytic mean/variance\n",
    "glm_grid = list(product(\n",
    "    prior_precisions,\n",
    "    temperatures,\n",
    "    hessian_structures,\n",
    "    ['glm'],       # pred_type\n",
    "    [None],        # n_samples\n",
    "    [False, True], # joint\n",
    "    [False, True]  # diagonal_output\n",
    "))\n",
    "\n",
    "# pred_type = 'gp' → FunctionalLLLaplace (requires separate setup)\n",
    "# Not included in default grid since it requires n_subset and a different class\n",
    "\n",
    "# Combine\n",
    "full_grid = nn_grid + glm_grid\n",
    "\n",
    "# Format as list of dicts\n",
    "hp_grid = [\n",
    "    {\n",
    "        'prior_precision': pp,\n",
    "        'temperature': temp,\n",
    "        'hessian_structure': hess,\n",
    "        'pred_type': pred,\n",
    "        'n_samples': n_samp,\n",
    "        'joint': joint,\n",
    "        'diagonal_output': diag_out,\n",
    "        'subset_of_weights': 'last_layer'\n",
    "    }\n",
    "    for pp, temp, hess, pred, n_samp, joint, diag_out in full_grid\n",
    "]\n",
    "\n",
    "print(f\"Total valid configurations: {len(hp_grid)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c98949",
   "metadata": {},
   "source": [
    "## 4. Utility: ECE computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f37117ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ece(probs, labels, n_bins=15):\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "    bins = torch.linspace(0, 1, n_bins + 1, device=probs.device)\n",
    "    ece = torch.zeros(1, device=probs.device)\n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences > bins[i]) & (confidences <= bins[i+1])\n",
    "        if mask.any():\n",
    "            ece += (mask.float().mean() * torch.abs(accuracies[mask].float().mean() - confidences[mask].mean()))\n",
    "    return ece.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b56c9c",
   "metadata": {},
   "source": [
    "## 5. One-at-a-time hyperparameter sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51bf2604",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 41\u001b[0m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m out \u001b[38;5;241m=\u001b[39m la(\n\u001b[0;32m     34\u001b[0m     x,\n\u001b[0;32m     35\u001b[0m     pred_type\u001b[38;5;241m=\u001b[39msettings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_type\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     diagonal_output\u001b[38;5;241m=\u001b[39msettings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagonal_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     40\u001b[0m )\n\u001b[1;32m---> 41\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     42\u001b[0m all_probs\u001b[38;5;241m.\u001b[39mappend(probs)\n\u001b[0;32m     43\u001b[0m all_targets\u001b[38;5;241m.\u001b[39mappend(y)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_ext = []\n",
    "for hp_name, values in hp_options.items():\n",
    "    for val in values:\n",
    "        # copy & overwrite this one hyperparam\n",
    "        settings = default_settings.copy()\n",
    "        settings[hp_name] = val\n",
    "\n",
    "        # if we're sweeping link_approx, move to a GLM predictor\n",
    "        if hp_name == 'link_approx':\n",
    "            settings['pred_type'] = 'glm'\n",
    "\n",
    "        # nn + non‐mc is invalid → skip\n",
    "        if settings['pred_type'] == 'nn' and settings['link_approx'] != 'mc':\n",
    "            continue\n",
    "\n",
    "        # now it's safe to build the Laplace\n",
    "        la = Laplace(\n",
    "            model, 'classification',\n",
    "            subset_of_weights=settings['subset_of_weights'],\n",
    "            hessian_structure=settings['hessian_structure'],\n",
    "            prior_precision=settings['prior_precision'],\n",
    "            temperature=settings['temperature']\n",
    "        )\n",
    "        la.fit(train_loader)\n",
    "\n",
    "        # predict + metrics\n",
    "        all_probs, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x = x.to(device)\n",
    "                out = la(\n",
    "                    x,\n",
    "                    pred_type=settings['pred_type'],\n",
    "                    link_approx=settings['link_approx'],\n",
    "                    n_samples=settings['n_samples'],\n",
    "                    joint=settings['joint'],\n",
    "                    diagonal_output=settings['diagonal_output']\n",
    "                )\n",
    "                probs = F.softmax(out['mean'], dim=1).cpu()\n",
    "                all_probs.append(probs)\n",
    "                all_targets.append(y)\n",
    "        all_probs = torch.cat(all_probs)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        nll = F.cross_entropy(torch.log(all_probs), all_targets).item()\n",
    "        acc = (all_probs.argmax(1) == all_targets).float().mean().item()\n",
    "        ece = compute_ece(all_probs, all_targets)\n",
    "\n",
    "        results_ext.append({\n",
    "            'hyperparam': hp_name,\n",
    "            'value':      val,\n",
    "            'nll':        nll,\n",
    "            'accuracy':   acc,\n",
    "            'ece':        ece,\n",
    "        })\n",
    "\n",
    "df_ext = pd.DataFrame(results_ext)\n",
    "df_ext.to_csv('/mnt/data/hyperparam_sensitivity_extended_results.csv', index=False)\n",
    "print('Extended results saved to /mnt/data/hyperparam_sensitivity_extended_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27959a",
   "metadata": {},
   "source": [
    "## 6. Plotting extended sensitivity results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e390ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_ext = pd.read_csv('/mnt/data/hyperparam_sensitivity_extended_results.csv')\n",
    "\n",
    "# Numeric hyperparameters to plot as curves\n",
    "for hp in ['prior_precision', 'temperature', 'n_samples']:\n",
    "    sub = df_ext[df_ext['hyperparam'] == hp]\n",
    "    plt.figure()\n",
    "    # Ensure numeric sorting\n",
    "    sub = sub.sort_values(by='value')\n",
    "    plt.plot(np.log10(sub['value']) if hp in ['prior_precision','temperature'] else sub['value'],\n",
    "             sub['nll'], marker='o')\n",
    "    plt.xlabel('log10(value)' if hp in ['prior_precision','temperature'] else hp)\n",
    "    plt.ylabel('NLL')\n",
    "    plt.title(f'NLL vs {hp}')\n",
    "    plt.show()\n",
    "\n",
    "# Categorical hyperparameters as bar charts\n",
    "for hp in ['hessian_structure', 'link_approx', 'joint', 'diagonal_output', 'pred_type', 'subset_of_weights']:\n",
    "    sub = df_ext[df_ext['hyperparam'] == hp]\n",
    "    mean_metrics = sub.groupby('value')[['nll','accuracy','ece']].mean()\n",
    "    mean_metrics.plot(kind='bar', subplots=True, layout=(1,3), figsize=(12,4), legend=False, sharex=True)\n",
    "    plt.suptitle(f'Metrics vs {hp}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
