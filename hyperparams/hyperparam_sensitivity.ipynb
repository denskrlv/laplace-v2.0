{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ca67f7",
   "metadata": {},
   "source": [
    "# Hyperparameter sensitivity experiment\n",
    "This notebook conducts a post-hoc Laplace approximation hyperparameter sensitivity analysis on a pre-trained WideResNet-16-4 model trained on CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6387e9",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Imports and model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f0fbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alberto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Alberto\\AppData\\Local\\Temp\\ipykernel_19336\\2976607064.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('models/wideresnet/pretrained/model_best.pth.tar', map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block1): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "sys.path.append('./')\n",
    "\n",
    "from laplace import Laplace\n",
    "from models.wideresnet.wideresnet import WideResNet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = WideResNet(depth=16, num_classes=10, widen_factor=4).to(device)\n",
    "checkpoint = torch.load('models/wideresnet/pretrained/model_best.pth.tar', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e4339",
   "metadata": {},
   "source": [
    "## 2. Data preparation\n",
    "Split CIFAR-10 into train/val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b91b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:11<00:00, 15.4MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
    "])\n",
    "full_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_set, val_set = random_split(full_train, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5f401",
   "metadata": {},
   "source": [
    "## 3. Default settings and hyperparameter options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2771483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings for one‐at‐a‐time sweeps\n",
    "default_settings = {\n",
    "    'prior_precision':    1.0,\n",
    "    'temperature':        1.0,\n",
    "    'hessian_structure':  'kron',\n",
    "    'link_approx':        'mc',            # ← mc is the only valid default for nn\n",
    "    'n_samples':          128,\n",
    "    'joint':              False,\n",
    "    'diagonal_output':    False,\n",
    "    'pred_type':          'nn',\n",
    "    'subset_of_weights':  'last_layer',\n",
    "}\n",
    "\n",
    "hp_options = {\n",
    "    'prior_precision':    torch.logspace(-6, 2, 20).tolist(),\n",
    "    'temperature':        torch.logspace(-1, 1, 10).tolist(),\n",
    "    'hessian_structure':  ['diag', 'kron', 'full', 'lowrank', 'gp'],\n",
    "    'link_approx':        ['probit', 'mc', 'bridge'],  # we'll switch pred_type when sweeping this\n",
    "    'n_samples':          [32, 128, 512],\n",
    "    'joint':              [False, True],\n",
    "    'diagonal_output':    [False, True],\n",
    "    'pred_type':          ['nn', 'glm', 'gp'],\n",
    "    'subset_of_weights':  ['last_layer', 'subnetwork', 'all'],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c98949",
   "metadata": {},
   "source": [
    "## 4. Utility: ECE computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f37117ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ece(probs, labels, n_bins=15):\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "    bins = torch.linspace(0, 1, n_bins + 1, device=probs.device)\n",
    "    ece = torch.zeros(1, device=probs.device)\n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences > bins[i]) & (confidences <= bins[i+1])\n",
    "        if mask.any():\n",
    "            ece += (mask.float().mean() * torch.abs(accuracies[mask].float().mean() - confidences[mask].mean()))\n",
    "    return ece.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b56c9c",
   "metadata": {},
   "source": [
    "## 5. One-at-a-time hyperparameter sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51bf2604",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 41\u001b[0m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m out \u001b[38;5;241m=\u001b[39m la(\n\u001b[0;32m     34\u001b[0m     x,\n\u001b[0;32m     35\u001b[0m     pred_type\u001b[38;5;241m=\u001b[39msettings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_type\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     diagonal_output\u001b[38;5;241m=\u001b[39msettings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagonal_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     40\u001b[0m )\n\u001b[1;32m---> 41\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     42\u001b[0m all_probs\u001b[38;5;241m.\u001b[39mappend(probs)\n\u001b[0;32m     43\u001b[0m all_targets\u001b[38;5;241m.\u001b[39mappend(y)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_ext = []\n",
    "for hp_name, values in hp_options.items():\n",
    "    for val in values:\n",
    "        # copy & overwrite this one hyperparam\n",
    "        settings = default_settings.copy()\n",
    "        settings[hp_name] = val\n",
    "\n",
    "        # if we're sweeping link_approx, move to a GLM predictor\n",
    "        if hp_name == 'link_approx':\n",
    "            settings['pred_type'] = 'glm'\n",
    "\n",
    "        # nn + non‐mc is invalid → skip\n",
    "        if settings['pred_type'] == 'nn' and settings['link_approx'] != 'mc':\n",
    "            continue\n",
    "\n",
    "        # now it's safe to build the Laplace\n",
    "        la = Laplace(\n",
    "            model, 'classification',\n",
    "            subset_of_weights=settings['subset_of_weights'],\n",
    "            hessian_structure=settings['hessian_structure'],\n",
    "            prior_precision=settings['prior_precision'],\n",
    "            temperature=settings['temperature']\n",
    "        )\n",
    "        la.fit(train_loader)\n",
    "\n",
    "        # predict + metrics\n",
    "        all_probs, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x = x.to(device)\n",
    "                out = la(\n",
    "                    x,\n",
    "                    pred_type=settings['pred_type'],\n",
    "                    link_approx=settings['link_approx'],\n",
    "                    n_samples=settings['n_samples'],\n",
    "                    joint=settings['joint'],\n",
    "                    diagonal_output=settings['diagonal_output']\n",
    "                )\n",
    "                probs = F.softmax(out['mean'], dim=1).cpu()\n",
    "                all_probs.append(probs)\n",
    "                all_targets.append(y)\n",
    "        all_probs = torch.cat(all_probs)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        nll = F.cross_entropy(torch.log(all_probs), all_targets).item()\n",
    "        acc = (all_probs.argmax(1) == all_targets).float().mean().item()\n",
    "        ece = compute_ece(all_probs, all_targets)\n",
    "\n",
    "        results_ext.append({\n",
    "            'hyperparam': hp_name,\n",
    "            'value':      val,\n",
    "            'nll':        nll,\n",
    "            'accuracy':   acc,\n",
    "            'ece':        ece,\n",
    "        })\n",
    "\n",
    "df_ext = pd.DataFrame(results_ext)\n",
    "df_ext.to_csv('/mnt/data/hyperparam_sensitivity_extended_results.csv', index=False)\n",
    "print('Extended results saved to /mnt/data/hyperparam_sensitivity_extended_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27959a",
   "metadata": {},
   "source": [
    "## 6. Plotting extended sensitivity results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e390ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_ext = pd.read_csv('/mnt/data/hyperparam_sensitivity_extended_results.csv')\n",
    "\n",
    "# Numeric hyperparameters to plot as curves\n",
    "for hp in ['prior_precision', 'temperature', 'n_samples']:\n",
    "    sub = df_ext[df_ext['hyperparam'] == hp]\n",
    "    plt.figure()\n",
    "    # Ensure numeric sorting\n",
    "    sub = sub.sort_values(by='value')\n",
    "    plt.plot(np.log10(sub['value']) if hp in ['prior_precision','temperature'] else sub['value'],\n",
    "             sub['nll'], marker='o')\n",
    "    plt.xlabel('log10(value)' if hp in ['prior_precision','temperature'] else hp)\n",
    "    plt.ylabel('NLL')\n",
    "    plt.title(f'NLL vs {hp}')\n",
    "    plt.show()\n",
    "\n",
    "# Categorical hyperparameters as bar charts\n",
    "for hp in ['hessian_structure', 'link_approx', 'joint', 'diagonal_output', 'pred_type', 'subset_of_weights']:\n",
    "    sub = df_ext[df_ext['hyperparam'] == hp]\n",
    "    mean_metrics = sub.groupby('value')[['nll','accuracy','ece']].mean()\n",
    "    mean_metrics.plot(kind='bar', subplots=True, layout=(1,3), figsize=(12,4), legend=False, sharex=True)\n",
    "    plt.suptitle(f'Metrics vs {hp}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
